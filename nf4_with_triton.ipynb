{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODwtKaxohXXaT05rvbLaOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan8912/Unsolth.ai-challenage/blob/main/nf4_with_triton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzOcxXtZoYp0",
        "outputId": "3b9d7d2a-927e-4563-d9c2-206a4a87abbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running NF4 dequantization benchmarks...\n",
            "\n",
            "Benchmarking NF4 Dequantization:\n",
            "============================================================\n",
            "       Shape |  Normal (ms) | Compiled (ms) |  Speedup\n",
            "------------------------------------------------------------\n",
            "10x     10 |      0.365 |      0.386 |    0.95x\n",
            "1024x     32 |      0.346 |      0.326 |    1.06x\n",
            "32x   1024 |      0.358 |      0.455 |    0.79x\n",
            "1024x   1024 |      0.385 |      0.361 |    1.07x\n",
            "4096x   4096 |      2.284 |      2.232 |    1.02x\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "import time\n",
        "\n",
        "@dataclass\n",
        "class NF4Config:\n",
        "    CLIP_MIN: int = -8\n",
        "    CLIP_MAX: int = 7\n",
        "    DTYPE_MIN: int = 0\n",
        "    DTYPE_MAX: int = 15\n",
        "\n",
        "class MemoryFormat:\n",
        "    CONTIGUOUS = \"contiguous\"\n",
        "    CHANNELS_LAST = \"channels_last\"\n",
        "\n",
        "@triton.jit\n",
        "def compute_absmax_kernel(\n",
        "    input_ptr,\n",
        "    absmax_ptr,\n",
        "    num_elements,\n",
        "    BLOCK_SIZE: tl.constexpr\n",
        "):\n",
        "    \"\"\"Compute absolute maximum values using efficient reduction.\"\"\"\n",
        "    pid = tl.program_id(0)\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < num_elements\n",
        "\n",
        "    # Load and compute absolute values\n",
        "    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n",
        "    x_abs = tl.abs(x)\n",
        "\n",
        "    # Perform reduction to find maximum\n",
        "    block_max = tl.max(x_abs, axis=0)\n",
        "\n",
        "    # Store result\n",
        "    tl.store(absmax_ptr + pid, block_max)\n",
        "\n",
        "@triton.jit\n",
        "def dequantize_kernel(\n",
        "    quantized_ptr,\n",
        "    absmax_ptr,\n",
        "    double_quant_scale_ptr,\n",
        "    output_ptr,\n",
        "    M, N,\n",
        "    stride_qm, stride_qn,\n",
        "    stride_om, stride_on,\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    MEMORY_FORMAT: tl.constexpr,\n",
        "    USE_DOUBLE_QUANT: tl.constexpr,\n",
        "):\n",
        "    \"\"\"Dequantize NF4 values with support for double quantization and different memory formats.\"\"\"\n",
        "    # Constants for NF4\n",
        "    NF4_CLIP_MIN = -8\n",
        "    NF4_CLIP_MAX = 7\n",
        "\n",
        "    # Program ID for 2D grid\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Calculate start indices\n",
        "    start_m = pid_m * BLOCK_M\n",
        "    start_n = pid_n * BLOCK_N\n",
        "\n",
        "    # Create ranges for the block\n",
        "    rm = start_m + tl.arange(0, BLOCK_M)\n",
        "    rn = start_n + tl.arange(0, BLOCK_N)\n",
        "\n",
        "    # Create masks for valid elements\n",
        "    mask_m = rm[:, None] < M\n",
        "    mask_n = rn[None, :] < N\n",
        "    mask = mask_m & mask_n\n",
        "\n",
        "    # Shared memory for frequently accessed scales\n",
        "    scale_cache = tl.zeros([BLOCK_M], dtype=tl.float32)\n",
        "\n",
        "    # Load quantized values based on memory format\n",
        "    if MEMORY_FORMAT == 1:  # channels_last\n",
        "        quantized = tl.load(\n",
        "            quantized_ptr + rm[:, None] * stride_qn + rn[None, :] * stride_qm,\n",
        "            mask=mask, other=0\n",
        "        )\n",
        "    else:  # contiguous\n",
        "        quantized = tl.load(\n",
        "            quantized_ptr + rm[:, None] * stride_qm + rn[None, :] * stride_qn,\n",
        "            mask=mask, other=0\n",
        "        )\n",
        "\n",
        "    # Load and cache absmax values\n",
        "    absmax = tl.load(absmax_ptr + rm, mask=rm < M, other=1.0)\n",
        "    scale_cache = absmax / NF4_CLIP_MAX\n",
        "\n",
        "    # Apply double quantization if enabled\n",
        "    if USE_DOUBLE_QUANT:\n",
        "        double_scale = tl.load(double_quant_scale_ptr + rm, mask=rm < M, other=1.0)\n",
        "        scale_cache = scale_cache * double_scale\n",
        "\n",
        "    # Dequantize\n",
        "    dequantized = (quantized - 8) * scale_cache[:, None]\n",
        "\n",
        "    # Store result\n",
        "    tl.store(\n",
        "        output_ptr + rm[:, None] * stride_om + rn[None, :] * stride_on,\n",
        "        dequantized,\n",
        "        mask=mask\n",
        "    )\n",
        "\n",
        "class NF4Dequantizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        use_double_quant: bool = True,\n",
        "        memory_format: str = MemoryFormat.CONTIGUOUS,\n",
        "        block_size: int = 1024,\n",
        "        compile_mode: str = \"reduce-overhead\"\n",
        "    ):\n",
        "        self.use_double_quant = use_double_quant\n",
        "        self.memory_format = memory_format\n",
        "        self.block_size = block_size\n",
        "        self.config = NF4Config()\n",
        "\n",
        "        # Compile the compute methods\n",
        "        self._compute_absmax_compiled = torch.compile(\n",
        "            self._compute_absmax_impl,\n",
        "            mode=compile_mode,\n",
        "            fullgraph=True\n",
        "        )\n",
        "        self._dequantize_compiled = torch.compile(\n",
        "            self._dequantize_impl,\n",
        "            mode=compile_mode,\n",
        "            fullgraph=True\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _compute_absmax_impl(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Implementation of absmax computation.\"\"\"\n",
        "        num_elements = input_tensor.numel()\n",
        "        num_blocks = (num_elements + self.block_size - 1) // self.block_size\n",
        "\n",
        "        absmax = torch.empty(num_blocks, device=input_tensor.device, dtype=torch.float32)\n",
        "\n",
        "        compute_absmax_kernel[(num_blocks,)](\n",
        "            input_tensor,\n",
        "            absmax,\n",
        "            num_elements,\n",
        "            BLOCK_SIZE=self.block_size\n",
        "        )\n",
        "\n",
        "        return absmax\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_absmax(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute absolute maximum values for input tensor using compiled implementation.\"\"\"\n",
        "        return self._compute_absmax_compiled(input_tensor)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequantize_impl(\n",
        "        self,\n",
        "        quantized_tensor: torch.Tensor,\n",
        "        absmax_tensor: torch.Tensor,\n",
        "        double_quant_scale: Optional[torch.Tensor]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Implementation of dequantization.\"\"\"\n",
        "        M, N = quantized_tensor.shape\n",
        "\n",
        "        output = torch.empty(\n",
        "            (M, N),\n",
        "            device=quantized_tensor.device,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        BLOCK_M, BLOCK_N = 128, 128\n",
        "        grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "        memory_format_int = 1 if self.memory_format == MemoryFormat.CHANNELS_LAST else 0\n",
        "\n",
        "        dequantize_kernel[grid](\n",
        "            quantized_tensor,\n",
        "            absmax_tensor,\n",
        "            double_quant_scale if double_quant_scale is not None else absmax_tensor,\n",
        "            output,\n",
        "            M, N,\n",
        "            quantized_tensor.stride(0), quantized_tensor.stride(1),\n",
        "            output.stride(0), output.stride(1),\n",
        "            BLOCK_M=BLOCK_M,\n",
        "            BLOCK_N=BLOCK_N,\n",
        "            MEMORY_FORMAT=memory_format_int,\n",
        "            USE_DOUBLE_QUANT=self.use_double_quant\n",
        "        )\n",
        "\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def dequantize(\n",
        "        self,\n",
        "        quantized_tensor: torch.Tensor,\n",
        "        absmax_tensor: Optional[torch.Tensor] = None,\n",
        "        double_quant_scale: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Dequantize NF4 tensor to fp16/bf16 using compiled implementation.\"\"\"\n",
        "        # Input validation\n",
        "        if not torch.is_tensor(quantized_tensor):\n",
        "            raise TypeError(\"quantized_tensor must be a torch.Tensor\")\n",
        "\n",
        "        if not quantized_tensor.is_cuda:\n",
        "            raise ValueError(\"quantized_tensor must be on CUDA device\")\n",
        "\n",
        "        if torch.any(quantized_tensor < self.config.DTYPE_MIN) or \\\n",
        "           torch.any(quantized_tensor > self.config.DTYPE_MAX):\n",
        "            raise ValueError(f\"Quantized values must be in range [{self.config.DTYPE_MIN}, {self.config.DTYPE_MAX}]\")\n",
        "\n",
        "        # Compute absmax if not provided\n",
        "        if absmax_tensor is None:\n",
        "            absmax_tensor = self.compute_absmax(quantized_tensor)\n",
        "\n",
        "        return self._dequantize_compiled(quantized_tensor, absmax_tensor, double_quant_scale)\n",
        "\n",
        "def benchmark_dequantizer(\n",
        "    shapes: list[Tuple[int, int]] = None,\n",
        "    num_warmup: int = 5,\n",
        "    num_runs: int = 100\n",
        "):\n",
        "    \"\"\"Benchmark the NF4Dequantizer with various matrix shapes.\"\"\"\n",
        "    if shapes is None:\n",
        "        shapes = [\n",
        "            (10, 10),      # Small square matrix\n",
        "            (1024, 32),    # Tall matrix\n",
        "            (32, 1024),    # Wide matrix\n",
        "            (1024, 1024),  # Large square matrix\n",
        "            (4096, 4096),  # Very large square matrix\n",
        "        ]\n",
        "\n",
        "    # Create dequantizers - one with compilation and one without\n",
        "    dequantizer_compiled = NF4Dequantizer(use_double_quant=True, compile_mode=\"reduce-overhead\")\n",
        "    dequantizer_normal = NF4Dequantizer(use_double_quant=True)\n",
        "\n",
        "    print(\"\\nBenchmarking NF4 Dequantization:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"{'Shape':>12} | {'Normal (ms)':>12} | {'Compiled (ms)':>12} | {'Speedup':>8}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for M, N in shapes:\n",
        "        # Generate test data\n",
        "        quantized = torch.randint(0, 16, (M, N), dtype=torch.int32, device='cuda')\n",
        "        absmax = torch.rand(M, device='cuda') * 10\n",
        "        double_quant_scale = torch.rand(M, device='cuda') * 2\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(num_warmup):\n",
        "            _ = dequantizer_normal.dequantize(quantized, absmax, double_quant_scale)\n",
        "            _ = dequantizer_compiled.dequantize(quantized, absmax, double_quant_scale)\n",
        "\n",
        "        # Benchmark normal version\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start.record()\n",
        "        for _ in range(num_runs):\n",
        "            _ = dequantizer_normal.dequantize(quantized, absmax, double_quant_scale)\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "        normal_time = start.elapsed_time(end) / num_runs\n",
        "\n",
        "        # Benchmark compiled version\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start.record()\n",
        "        for _ in range(num_runs):\n",
        "            _ = dequantizer_compiled.dequantize(quantized, absmax, double_quant_scale)\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "        compiled_time = start.elapsed_time(end) / num_runs\n",
        "\n",
        "        # Calculate speedup\n",
        "        speedup = normal_time / compiled_time\n",
        "\n",
        "        print(f\"{M}x{N:>7} | {normal_time:>10.3f} | {compiled_time:>10.3f} | {speedup:>7.2f}x\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running NF4 dequantization benchmarks...\")\n",
        "    benchmark_dequantizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vFR0KUToZ6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}