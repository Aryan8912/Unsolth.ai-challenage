{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOIqjN942Dm92kxA4SKzx4f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan8912/Unsolth.ai-challenage/blob/main/nf4_with_triton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzOcxXtZoYp0",
        "outputId": "8d7b4d1b-6963-4f25-bf22-662fe3c54fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running NF4 dequantization tests...\n",
            "\n",
            "Testing shape: 10x10\n",
            "✓ Dequantization successful\n",
            "✓ Output shape: torch.Size([10, 10])\n",
            "✓ Processing time: 5456.33 ms\n",
            "\n",
            "Testing shape: 1024x32\n",
            "✓ Dequantization successful\n",
            "✓ Output shape: torch.Size([1024, 32])\n",
            "✓ Processing time: 412.36 ms\n",
            "\n",
            "Testing shape: 32x1024\n",
            "✓ Dequantization successful\n",
            "✓ Output shape: torch.Size([32, 1024])\n",
            "✓ Processing time: 0.42 ms\n",
            "\n",
            "Testing shape: 1x1024\n",
            "✓ Dequantization successful\n",
            "✓ Output shape: torch.Size([1, 1024])\n",
            "✓ Processing time: 681.82 ms\n",
            "\n",
            "Testing shape: 1024x1\n",
            "✓ Dequantization successful\n",
            "✓ Output shape: torch.Size([1024, 1])\n",
            "✓ Processing time: 649.02 ms\n",
            "\n",
            "Testing shape: 256x256\n",
            "✓ Dequantization successful\n",
            "✓ Output shape: torch.Size([256, 256])\n",
            "✓ Processing time: 0.39 ms\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "import time\n",
        "\n",
        "@dataclass\n",
        "class NF4Config:\n",
        "    CLIP_MIN: int = -8\n",
        "    CLIP_MAX: int = 7\n",
        "    DTYPE_MIN: int = 0\n",
        "    DTYPE_MAX: int = 15\n",
        "\n",
        "class MemoryFormat:\n",
        "    CONTIGUOUS = \"contiguous\"\n",
        "    CHANNELS_LAST = \"channels_last\"\n",
        "\n",
        "@triton.jit\n",
        "def compute_absmax_kernel(\n",
        "    input_ptr,\n",
        "    absmax_ptr,\n",
        "    num_elements,\n",
        "    BLOCK_SIZE: tl.constexpr\n",
        "):\n",
        "    \"\"\"Compute absolute maximum values using efficient reduction.\"\"\"\n",
        "    pid = tl.program_id(0)\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < num_elements\n",
        "\n",
        "    # Load and compute absolute values\n",
        "    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n",
        "    x_abs = tl.abs(x)\n",
        "\n",
        "    # Perform reduction to find maximum\n",
        "    block_max = tl.max(x_abs, axis=0)\n",
        "\n",
        "    # Store result\n",
        "    tl.store(absmax_ptr + pid, block_max)\n",
        "\n",
        "@triton.jit\n",
        "def dequantize_kernel(\n",
        "    quantized_ptr,\n",
        "    absmax_ptr,\n",
        "    double_quant_scale_ptr,\n",
        "    output_ptr,\n",
        "    M, N,\n",
        "    stride_qm, stride_qn,\n",
        "    stride_om, stride_on,\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    MEMORY_FORMAT: tl.constexpr,\n",
        "    USE_DOUBLE_QUANT: tl.constexpr,\n",
        "):\n",
        "    \"\"\"Dequantize NF4 values with support for double quantization and different memory formats.\"\"\"\n",
        "    # Constants for NF4\n",
        "    NF4_CLIP_MIN = -8\n",
        "    NF4_CLIP_MAX = 7\n",
        "\n",
        "    # Program ID for 2D grid\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Calculate start indices\n",
        "    start_m = pid_m * BLOCK_M\n",
        "    start_n = pid_n * BLOCK_N\n",
        "\n",
        "    # Create ranges for the block\n",
        "    rm = start_m + tl.arange(0, BLOCK_M)\n",
        "    rn = start_n + tl.arange(0, BLOCK_N)\n",
        "\n",
        "    # Create masks for valid elements\n",
        "    mask_m = rm[:, None] < M\n",
        "    mask_n = rn[None, :] < N\n",
        "    mask = mask_m & mask_n\n",
        "\n",
        "    # Shared memory for frequently accessed scales\n",
        "    scale_cache = tl.zeros([BLOCK_M], dtype=tl.float32)\n",
        "\n",
        "    # Load quantized values based on memory format\n",
        "    if MEMORY_FORMAT == 1:  # channels_last\n",
        "        quantized = tl.load(\n",
        "            quantized_ptr + rm[:, None] * stride_qn + rn[None, :] * stride_qm,\n",
        "            mask=mask, other=0\n",
        "        )\n",
        "    else:  # contiguous\n",
        "        quantized = tl.load(\n",
        "            quantized_ptr + rm[:, None] * stride_qm + rn[None, :] * stride_qn,\n",
        "            mask=mask, other=0\n",
        "        )\n",
        "\n",
        "    # Load and cache absmax values\n",
        "    absmax = tl.load(absmax_ptr + rm, mask=rm < M, other=1.0)\n",
        "    scale_cache = absmax / NF4_CLIP_MAX\n",
        "\n",
        "    # Apply double quantization if enabled\n",
        "    if USE_DOUBLE_QUANT:\n",
        "        double_scale = tl.load(double_quant_scale_ptr + rm, mask=rm < M, other=1.0)\n",
        "        scale_cache = scale_cache * double_scale\n",
        "\n",
        "    # Dequantize\n",
        "    dequantized = (quantized - 8) * scale_cache[:, None]\n",
        "\n",
        "    # Store result\n",
        "    tl.store(\n",
        "        output_ptr + rm[:, None] * stride_om + rn[None, :] * stride_on,\n",
        "        dequantized,\n",
        "        mask=mask\n",
        "    )\n",
        "\n",
        "class NF4Dequantizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        use_double_quant: bool = True,\n",
        "        memory_format: str = MemoryFormat.CONTIGUOUS,\n",
        "        block_size: int = 1024\n",
        "    ):\n",
        "        self.use_double_quant = use_double_quant\n",
        "        self.memory_format = memory_format\n",
        "        self.block_size = block_size\n",
        "        self.config = NF4Config()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_absmax(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute absolute maximum values for input tensor.\"\"\"\n",
        "        num_elements = input_tensor.numel()\n",
        "        num_blocks = (num_elements + self.block_size - 1) // self.block_size\n",
        "\n",
        "        absmax = torch.empty(num_blocks, device=input_tensor.device, dtype=torch.float32)\n",
        "\n",
        "        compute_absmax_kernel[(num_blocks,)](\n",
        "            input_tensor,\n",
        "            absmax,\n",
        "            num_elements,\n",
        "            BLOCK_SIZE=self.block_size\n",
        "        )\n",
        "\n",
        "        return absmax\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def dequantize(\n",
        "        self,\n",
        "        quantized_tensor: torch.Tensor,\n",
        "        absmax_tensor: Optional[torch.Tensor] = None,\n",
        "        double_quant_scale: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Dequantize NF4 tensor to fp16/bf16.\"\"\"\n",
        "        # Input validation\n",
        "        if not torch.is_tensor(quantized_tensor):\n",
        "            raise TypeError(\"quantized_tensor must be a torch.Tensor\")\n",
        "\n",
        "        if not quantized_tensor.is_cuda:\n",
        "            raise ValueError(\"quantized_tensor must be on CUDA device\")\n",
        "\n",
        "        if torch.any(quantized_tensor < self.config.DTYPE_MIN) or \\\n",
        "           torch.any(quantized_tensor > self.config.DTYPE_MAX):\n",
        "            raise ValueError(f\"Quantized values must be in range [{self.config.DTYPE_MIN}, {self.config.DTYPE_MAX}]\")\n",
        "\n",
        "        # Get tensor dimensions\n",
        "        M, N = quantized_tensor.shape\n",
        "\n",
        "        # Compute or validate absmax\n",
        "        if absmax_tensor is None:\n",
        "            absmax_tensor = self.compute_absmax(quantized_tensor)\n",
        "\n",
        "        # Prepare output tensor\n",
        "        output = torch.empty(\n",
        "            (M, N),\n",
        "            device=quantized_tensor.device,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # Define block sizes for the kernel\n",
        "        BLOCK_M, BLOCK_N = 128, 128\n",
        "\n",
        "        # Calculate grid dimensions\n",
        "        grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "        # Convert memory format to integer for kernel\n",
        "        memory_format_int = 1 if self.memory_format == MemoryFormat.CHANNELS_LAST else 0\n",
        "\n",
        "        # Launch kernel\n",
        "        dequantize_kernel[grid](\n",
        "            quantized_tensor,\n",
        "            absmax_tensor,\n",
        "            double_quant_scale if double_quant_scale is not None else absmax_tensor,  # Fallback\n",
        "            output,\n",
        "            M, N,\n",
        "            quantized_tensor.stride(0), quantized_tensor.stride(1),\n",
        "            output.stride(0), output.stride(1),\n",
        "            BLOCK_M=BLOCK_M,\n",
        "            BLOCK_N=BLOCK_N,\n",
        "            MEMORY_FORMAT=memory_format_int,\n",
        "            USE_DOUBLE_QUANT=self.use_double_quant\n",
        "        )\n",
        "\n",
        "        return output\n",
        "\n",
        "def test_dequantizer(shapes: list[Tuple[int, int]] = None):\n",
        "    \"\"\"Test the NF4Dequantizer with various matrix shapes.\"\"\"\n",
        "    if shapes is None:\n",
        "        shapes = [\n",
        "            (10, 10),      # Small square matrix\n",
        "            (1024, 32),    # Tall matrix\n",
        "            (32, 1024),    # Wide matrix\n",
        "            (1, 1024),     # Single row\n",
        "            (1024, 1),     # Single column\n",
        "            (256, 256)     # Medium square matrix\n",
        "        ]\n",
        "\n",
        "    dequantizer = NF4Dequantizer(use_double_quant=True)\n",
        "\n",
        "    for M, N in shapes:\n",
        "        print(f\"\\nTesting shape: {M}x{N}\")\n",
        "\n",
        "        # Generate test data\n",
        "        quantized = torch.randint(\n",
        "            0, 16, (M, N),\n",
        "            dtype=torch.int32,\n",
        "            device='cuda'\n",
        "        )\n",
        "        absmax = torch.rand(M, device='cuda') * 10\n",
        "        double_quant_scale = torch.rand(M, device='cuda') * 2\n",
        "\n",
        "        try:\n",
        "            # Time the dequantization\n",
        "            start_event = torch.cuda.Event(enable_timing=True)\n",
        "            end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "            start_event.record()\n",
        "            output = dequantizer.dequantize(\n",
        "                quantized,\n",
        "                absmax,\n",
        "                double_quant_scale\n",
        "            )\n",
        "            end_event.record()\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            elapsed_time = start_event.elapsed_time(end_event)\n",
        "\n",
        "            print(f\"✓ Dequantization successful\")\n",
        "            print(f\"✓ Output shape: {output.shape}\")\n",
        "            print(f\"✓ Processing time: {elapsed_time:.2f} ms\")\n",
        "\n",
        "            # Basic output validation\n",
        "            assert not torch.isnan(output).any(), \"Output contains NaN values\"\n",
        "            assert not torch.isinf(output).any(), \"Output contains Inf values\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Test failed: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    print(\"Running NF4 dequantization tests...\")\n",
        "    test_dequantizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vFR0KUToZ6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}